# -*- coding: utf-8 -*-
"""heart rate prediction project main

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q6NB7ILs6bNrNqVdEUTsoZIGKbpG5zS7

# HEART DISEASE PREDICTION USING MACHINE LEARNING

**Team Member Names :-**

Potnuru Lavanya

Kotti Jahanvi

# Problem Statement

The aim of this project is to develop a supervised machine learning model that accurately predicts the presence of heart disease in patients using various medical features such as age, gender, chest pain type, cholesterol levels, maximum heart rate, and exercise-induced angina. The project involves data preprocessing, feature analysis, model selection, hyperparameter tuning, and evaluation using metrics like accuracy, precision, recall, and F1-score. The final goal is to create a reliable predictive tool that supports early diagnosis and aids clinical decision-making.

# Project WorkFlow

1. Data Preprocessing

2. Feature Analysis

3. Model Selection

4. Hyperparameter Tuning

5. Model Evaluation

6. Deployment

# Data Preprocessing

# Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""# Loading the Dataset"""

df1= pd.read_csv("/content/heart rate.csv")
df1

"""# Dataset Explanation

| Column Name | Description                         | Example Values                                                                        | Type                               | Why This Type                           |
| ----------- | ----------------------------------- | ------------------------------------------------------------------------------------- | ---------------------------------- | --------------------------------------- |
| `age`       | Age of the patient                  | 29, 45, 63                                                                            | **Numerical**                      | Continuous numeric value (years)        |
| `sex`       | Gender of the patient               | 0 = Female, 1 = Male                                                                  | **Categorical (Binary)**                    | Fixed categories (binary value)         |
| `cp`        | Chest pain type                     | 0 = Typical angina<br>1 = Atypical angina<br>2 = Non-anginal pain<br>3 = Asymptomatic | **Categorical**                    | Fixed number of groups coded as numbers |
| `trtbps`    | Resting blood pressure (mm Hg)      | 110, 130, 150                                                                         | **Numerical**                      | Measured value with continuous range    |
| `chol`      | Serum cholesterol (mg/dl)           | 180, 240, 300                                                                         | **Numerical**                      | Measured blood cholesterol level        |
| `fbs`       | Fasting blood sugar > 120 mg/dl     | 0 = False, 1 = True                                                                   | **Categorical (Binary)**                    | Binary indicator for high sugar         |
| `restecg`   | Resting ECG(Electrocardiogram) results                 | 0 = Normal<br>1 = ST-T abnormality<br>2 = LV hypertrophy                              | **Categorical**                    | Coded categories of ECG results         |
| `thalachh`  | Max heart rate achieved             | 150, 170, 190                                                                         | **Numerical**                      | Real number (bpm), continuous           |
| `exng`      | Exercise-induced angina             | 0 = No, 1 = Yes                                                                       | **Categorical (Binary)**                    | Binary yes/no category                  |
| `oldpeak`   | ST depression from rest (ECG)(mm)       | 0.0, 1.4, 2.3                                                                         | **Numerical**                      | Decimal values measuring depression     |
| `slp`       | Slope of ST segment during exercise | 0 = Upsloping<br>1 = Flat<br>2 = Downsloping                                          | **Categorical**                    | Coded ECG slope categories              |
| `caa`       | No. of major vessels seen (0–3)     | 0, 1, 2, 3                                                                            | **Categorical**                    | Small number of fixed values            |
| `thall`     | Thalassemia (blood disorder) type   | 1 = Normal<br>2 = Fixed defect<br>3 = Reversible defect                               | **Categorical**                    | Categories of thalassemia condition     |
| `output`    | Target: Heart disease present       | 0 = No disease<br>1 = Disease present                                                 | **Target (Binary Classification)** | What we are trying to predict           |
"""

df=df1.copy()
df

"""# Import Warnings"""

import warnings
warnings.filterwarnings(action='ignore',category=FutureWarning)

df.shape

df.size

"""# Confirm the data has been correctly by displaying the first 5 and last 5 records."""

df.head()

df.tail()

"""# Display the column headings, statistical information, description and statistical summary of the data."""

df.info()

df.columns

df.describe()



df.describe(include='all')

"""#checking null values"""

df.isnull().sum()

df.isnull().mean()*100

df.dtypes

"""# Null values are filled using mode and median

We use median to fill missing numerical values because it is robust to outliers and preserves the original data distribution without being skewed. For categorical or discrete features, we use the mode as it maintains the most common category and avoids introducing rare or artificial values. Both methods help retain the natural structure of the dataset and prevent bias or imbalance during model training.
"""

# Columns that are numeric but represent categories
mode_cols = ['cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall']
for col in mode_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

#columns that are numeric
median_cols = ['trtbps', 'chol', 'thalachh', 'oldpeak']
for col in median_cols:
    df[col] = df[col].fillna(df[col].median())


print(" Missing values after imputation:")
print(df.isnull().sum())

"""# Converted float values into integers"""

# Keep oldpeak as float, convert other float columns to int
for col in df.columns:
    if df[col].dtype == 'float64' and col != 'oldpeak':
        df[col] = df[col].astype(int)

df.info()

df

"""# Check whether the dataset contains outliers"""

for column in df.select_dtypes(include=['number']).columns:
  plt.figure(figsize=(6,4))
  sns.boxplot(y=df[column])

"""# Count outliers in each column using IQR"""

for col in df.columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    outliers = df[(df[col] < lower) | (df[col] > upper)]
    print(f"{col}: {len(outliers)} outliers")

"""# Handled outliers using IQR method

We used the IQR (Interquartile Range) method to handle outliers because it helps detect and remove extreme values that fall far outside the normal range of the data.
This makes the model more accurate and prevents it from being influenced by unusually high or low values.
"""

# Columns to cap outliers
cap_cols = ['trtbps', 'chol', 'thalachh', 'oldpeak', 'caa', 'thall']

for col in cap_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    df[col] = df[col].clip(lower=lower, upper=upper)

"""# Rechecking the outliers that handled or not"""

for column in df.select_dtypes(include=['number']).columns:
  plt.figure(figsize=(6,4))
  sns.boxplot(y=df[column])

"""# Data Exploration using various plots

# What is the distribution of heart disease by gender?
"""

sns.countplot(x='output',hue='sex' ,data=df, palette='viridis')
plt.title("Target Variable Distribution (Heart Disease Presence)")
plt.xlabel("Output (0= No Disease, 1 = Disease)")
plt.ylabel("Count")
plt.show()

"""Insight:

--> The count plot shows that that there is a higher number of male(1) without heart disease (output = 0) compared to female (0).

-->And males(1) are also high patients with heart disease(output = 1) rather than female (0).

# What is the gender distribution of patients?
"""

gender_counts = df['sex'].value_counts().sort_index()
labels = ['Female', 'Male']
colors = ['Red', 'Yellow']
plt.figure(figsize=(6,6))
plt.pie(gender_counts, labels=labels, autopct='%1.1f%%', colors=colors, startangle=140)
plt.title("Gender Distribution")
plt.show()

"""Insight:

The above pie chart represent highest percentage of male patients are there compared to female patients.

# Which age range shows the highest prevalence of heart disease?
"""

plt.figure(figsize=(8, 5))
sns.histplot(data=df, x='age', hue='output', multiple='stack', kde=True, palette='husl')
plt.title('Age Distribution by Heart Disease Status')
plt.show()

"""Insight:

Heart disease is more common in individuals aged 45–60, as seen by the taller stacked bars for class 1

# How does cholesterol level vary between patients with and without heart disease?
"""

output = df.groupby('output')['chol'].mean().reset_index()
plt.figure(figsize=(6,4))
sns.barplot(x='output', y='chol', data=output, palette='Set2')
plt.title("Average Cholesterol by Heart Disease Output")
plt.xlabel("Heart Disease (0 = No, 1 = Yes)")
plt.ylabel("Average Cholesterol")
plt.show()

"""Insight:

People with heart disease have a lower average cholesterol level than those without heart disease.

# Which features are strongly correlated with heart disease?
"""

plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap of Features")
plt.show()

"""Insight:

--> maximum heart rate (thalachh) and output show a positive correlation with heart disease.

--> caa and output have negatively correlated

# How does resting blood pressure (trtbps) vary with chest pain type (cp) across different genders (sex) ?
"""

data=df.groupby(['cp','sex'])['trtbps'].mean()
data

"""Insight:

--> For Angina chest pain ,females have higher resting blood pressure.

--> For Atypical Angina chest pain,both are equal in trtbps.

--> For NonAnginal pain,males have higher trtbps.

--> For Asymptomatic pain,females have higher trtbps.

# Find the Average Heart Rate by Gender
"""

data=df.groupby('sex')['thalachh'].mean()
data

sns.barplot(x='sex', y='thalachh', data=df,palette='husl')
plt.title("Average Max Heart Rate by Gender")
plt.xlabel("Sex (0 = Female, 1 = Male)")
plt.ylabel("thalachh")
plt.show()

"""Insight:

--> The plot shows that females (sex = 0) have a higher average maximum heart rate than males (sex = 1).

--> This suggests gender may play a role in heart rate patterns, which could be considered when analyzing heart disease risk

# How many patients without heart disease have exercise-induced angina?
"""

filtered_df = (df['output'] == 0)
plt.figure(figsize=(8,6))
sns.violinplot(x='exng', y='thalachh', data=df[df['output'] == 0], palette='Set2')
plt.title("Max Heart Rate of Patients without Heart Disease")
plt.xlabel("Exercise-induced Angina (0 = No, 1 = Yes)")
plt.ylabel("Maximum Heart Rate")
plt.show()

"""Insight:

--> Patients without angina (exng = 0) generally have a higher maximum heart rate.

-->Those with angina (exng = 1) tend to show lower heart rate response during exercise.

# What is the average cholesterol level for each type of chest pain?
"""

plt.figure(figsize=(8, 5))
grouped_df = df.groupby('cp')['chol'].mean().reset_index()
sns.lineplot(x='cp', y='chol', data=grouped_df,marker='o', color='teal')
#sns.lineplot(x='cp', y='chol', data=df,marker='o', color='teal')
plt.title('Average Cholesterol by Chest Pain Type')
plt.xlabel('Chest Pain Type (cp)')
plt.ylabel('Average Cholesterol (chol)')
plt.grid(True)
plt.show()

"""Insight:

-->Each point = one chest pain category

X-axis (cp):

0 → Typical Angina

1 → Atypical Angina

2 → Non-anginal Pain

3 → Asymptomatic

-->  we say "chest pain types 0 and 1 have higher average cholesterol", that referring to the first two points on the x-axis of the plot

# How is heart disease distributed among patients with low and high maximum heart rates?
"""

plt.figure(figsize=(10, 6))
sns.histplot(data=df, x='thalachh', hue='output', kde=True, palette='viridis')
plt.title('Distribution of Maximum Heart Rate by Heart Disease')
plt.xlabel('Maximum Heart Rate (thalachh)')
plt.ylabel('Frequency')
plt.show()

"""Insight:

People without heart disease have lower peak heart rates, possibly due to poor cardiovascular fitness or blocked arteries.Dark green shows both groups are overlapped

# Do people with heart disease have higher blood pressure and cholesterol than those without?
"""

grouped = df.groupby('output')[['trtbps', 'chol']].mean()
print(grouped)

"""# Insight:

People without heart disease actually have higher average blood pressure and cholesterol than those with heart disease.
"""



"""# Find the Target column and drop it while Splitting the data into training and testing and also do scaling

We used StandardScaler instead of MinMaxScaler because most of your features are normally distributed or have outliers. StandardScaler transforms data to have mean = 0 and standard deviation = 1, which works well with models like Logistic Regression, SVM, and Decision Trees that are less sensitive to the data’s absolute range.
"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# drop target column
X = df.drop('output', axis=1)
y = df['output']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Implement Machine Learning Models

#1. Logistic Regression

Logistic Regression is a supervised machine learning algorithm used for binary classification problems.
It predicts the probability of an outcome ( Yes/No, 0/1, or Disease/No Disease.) using a sigmoid function.
Unlike linear regression, it outputs values between 0 and 1, which are then converted to class labels.

# Why We Use Logistic Regression:

It’s easy to interpret

Works well for binary problems like heart disease prediction (0 = No disease, 1 = Disease)

Tells us how each feature impacts the prediction (positive or negative)

Fast to train and requires less computing power

Provides probabilities, not just labels

# Model without hyperparameters
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
model= LogisticRegression(random_state=55)
model.fit(X_train_scaled, y_train)

#  Predict on full dataset
X_full_scaled = scaler.transform(X)
train_preds = model.predict(X_train_scaled)
test_preds = model.predict(X_test_scaled)
y_full_pred = model.predict(X_full_scaled)

#  Evaluate on full dataset (303 records)
print(" Training Accuracy:", accuracy_score(y_train, train_preds))
print(" Test Accuracy:", accuracy_score(y_test, test_preds))
print(" Classification Report on FULL dataset (303 rows):")
print(classification_report(y, y_full_pred))

print(" Confusion Matrix:")
print(confusion_matrix(y, y_full_pred))

"""# Insight :-

The logistic regression model is performing very well, with balanced accuracy, no overfitting, and a low, evenly distributed error rate. It’s a reliable model for this dataset.

# Model with Hyper parameters
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
model = LogisticRegression(
    penalty='l2',
    C=0.07,
    solver='liblinear',
    class_weight='balanced',
    max_iter=1000,
    random_state=3
)
model.fit(X_train_scaled, y_train)
X_full_scaled = scaler.transform(X)

train_preds = model.predict(X_train_scaled)
test_preds = model.predict(X_test_scaled)

#  Predict on full dataset
y_full_pred = model.predict(X_full_scaled)

#  Evaluate on full dataset (303 records)
print(" Training Accuracy:", accuracy_score(y_train, train_preds))
print(" Test Accuracy:", accuracy_score(y_test, test_preds))
print(" Classification Report on FULL dataset (303 rows):")
print(classification_report(y, y_full_pred))

print(" Confusion Matrix:")
print(confusion_matrix(y, y_full_pred))

"""# Insight :-


logistic regression model achieves high accuracy (86%) with a minimal train-test gap, indicating strong generalization.
Both classes have balanced precision and recall, making the model reliable for predicting heart disease risk.
With a low misclassification rate and strong F1-scores, this model is well-suited for real-world medical applications.

#2. Decision Tree

A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks.
It works like a flowchart: at each step, the data is split based on a condition (a feature), and this continues until the model makes a final decision (prediction).

# Why We Use Decision Trees:

Simple and interpretable model

Fast to train and test

Good for feature importance analysis

Works well on small to medium-sized datasets

#Model without Hyper Parameters
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
dt_model1=DecisionTreeClassifier(random_state=30)
dt_model1.fit(X_train_scaled, y_train)

#  Predict on full dataset
X_full_scaled = scaler.transform(X)
train_preds = dt_model1.predict(X_train_scaled)
test_preds = dt_model1.predict(X_test_scaled)
y_full_pred = dt_model1.predict(X_full_scaled)

#  Evaluate on full dataset (303 records)
print(" Training Accuracy:", accuracy_score(y_train, train_preds))
print(" Test Accuracy:", accuracy_score(y_test, test_preds))
print(" Classification Report on FULL dataset (303 rows):")
print(classification_report(y, y_full_pred))

print(" Confusion Matrix:")
print(confusion_matrix(y, y_full_pred))

"""# Insight :-

The decision tree without any hyperparameter tuning is overfitting the training data. It learns every small detail in the training set (even noise), which results in perfect training accuracy, but performs poorly on new data

# Model with Hyperparameters
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Final tuned Decision Tree

dt_model1 = DecisionTreeClassifier(
   criterion='gini',
    max_depth=4,
    min_samples_split=12,
    min_samples_leaf=6,
    max_leaf_nodes=12,
    splitter='random',
    class_weight='balanced',
    random_state=3
)

dt_model1.fit(X_train_scaled, y_train)

#  Predict on full dataset
X_full_scaled = scaler.transform(X)
train_preds = dt_model1.predict(X_train_scaled)
test_preds = dt_model1.predict(X_test_scaled)
y_full_pred = dt_model1.predict(X_full_scaled)

#  Evaluate on full dataset (303 records)
print(" Training Accuracy:", accuracy_score(y_train, train_preds))
print(" Test Accuracy:", accuracy_score(y_test, test_preds))
print(" Classification Report on FULL dataset (303 rows):")
print(classification_report(y, y_full_pred))

print(" Confusion Matrix:")
print(confusion_matrix(y, y_full_pred))

"""# Insight :-

 It performs well in identifying heart disease cases with a high recall of 90% and a balanced precision of 86%. The model maintains strong overall performance with an F1-score of 88% for patients with disease. This makes it a reliable and interpretable model for early heart disease prediction.

# sample prediction using Decision Tree
"""

features = ['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg',
            'thalachh', 'exng', 'oldpeak', 'slope', 'ca', 'thal']
import numpy as np
import warnings
warnings.filterwarnings(action='ignore',category=FutureWarning)
sample_data = np.array([[52, 1, 0, 125, 212, 0, 1, 168, 0, 1.0, 2, 1, 2]])
prediction = dt_model1.predict(sample_data)
print("Predicted class:", prediction[0])


probability = dt_model1.predict_proba(sample_data)
print("Probability of No Disease:", f"{probability[0][0]:.2f}")
print("Probability of Disease:", f"{probability[0][1]:.2f}")

"""# Result :-

The model predicts the patient does not have heart disease (class 0).
It gives a 76% probability of no disease and 24% probability of disease.
This indicates a low-risk profile, but some risk still exists.

#3. Random Forest

Random Forest is an ensemble machine learning algorithm that builds many decision trees and combines their results to make a final prediction.
It reduces overfitting and increases accuracy by using the majority vote (for classification) from all trees.

# Why We Used Random Forest:

It works well with both numerical and categorical data

It's robust to outliers and missing values

It reduces overfitting compared to a single decision tree

It gives feature importance, which helps understand the dataset

It provides high accuracy and stability for medical data

# Model without Hyperparameters
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_scaled, y_train)

#  Predict on full dataset
X_full_scaled = scaler.transform(X)
train_preds = rf_model.predict(X_train_scaled)
test_preds = rf_model.predict(X_test_scaled)
y_full_pred = rf_model.predict(X_full_scaled)

#  Evaluate on full dataset (303 records)
print(" Training Accuracy:", accuracy_score(y_train, train_preds))
print(" Test Accuracy:", accuracy_score(y_test, test_preds))
print(" Classification Report on FULL dataset (303 rows):")
print(classification_report(y, y_full_pred))

print(" Confusion Matrix:")
print(confusion_matrix(y, y_full_pred))

"""# Insight :-

The model achieved 100% training accuracy but only 78.6% on test data, indicating overfitting.
On the full dataset, it performed well with 96% accuracy and high precision and recall.
Only 13 misclassifications out of 303 records, showing strong but possibly overfit performance.

# Model with Hyper parameters
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=2,
    min_samples_split=25,
    min_samples_leaf=10,
    max_features='sqrt',
    bootstrap=True,
    random_state=42
)

rf_model.fit(X_train_scaled, y_train)

#  Predict on full dataset
X_full_scaled = scaler.transform(X)
train_preds = rf_model.predict(X_train_scaled)
test_preds = rf_model.predict(X_test_scaled)
y_full_pred = rf_model.predict(X_full_scaled)

#  Evaluate on full dataset (303 records)
print(" Training Accuracy:", accuracy_score(y_train, train_preds))
print(" Test Accuracy:", accuracy_score(y_test, test_preds))
print(" Classification Report on FULL dataset (303 rows):")
print(classification_report(y, y_full_pred))

print(" Confusion Matrix:")
print(confusion_matrix(y, y_full_pred))

"""# Insight :-

After tuning, your random forest shows balanced train (85.5%) and test (83.6%) accuracy, meaning it generalizes well.
It performs strongly on both classes with good precision and recall, especially detecting disease cases (recall = 87%).
This is now a well-optimized and reliable model suitable for real-world predictions.

# sample prediction using Random Forest
"""

features = ['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg',
            'thalachh', 'exng', 'oldpeak', 'slope', 'caa', 'thal']
import numpy as np
import warnings
warnings.filterwarnings(action='ignore',category=FutureWarning)
sample_data = np.array([[52, 1, 0, 125, 212, 0, 1, 168, 0, 1.0, 2, 1, 2]])
prediction = rf_model.predict(sample_data)
print("Predicted class:", prediction[0])


probability = rf_model.predict_proba(sample_data)
print("Probability of No Disease:", f"{probability[0][0]:.2f}")
print("Probability of Disease:", f"{probability[0][1]:.2f}")

"""# Result :-

 The model predicts no disease, but it’s not very sure (only 52% confident).
The chances of disease and no disease are almost equal.It’s a confusing case.

#4. Gradient Boosting

Gradient Boosting is an advanced machine learning technique used for classification and regression problems.
It builds a series of weak models (usually decision trees), where each model tries to fix the errors made by the previous one.
Over time, these models are combined to create a strong and accurate final model.

# Why We Use Gradient Boosting:

It's great for complex datasets

Often performs better than simple models like logistic regression or a single decision tree

Can be tuned using parameters like n_estimators, learning_rate, and max_depth

Works well even with imbalanced or noisy data

# Model without Hyper parameters
"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. Create the model
gb_model = GradientBoostingClassifier(random_state=40)

# 2. Train the model
X_full_scaled = scaler.transform(X)
gb_model.fit(X_train_scaled, y_train)
train_preds = gb_model.predict(X_train_scaled)
test_preds = gb_model.predict(X_test_scaled)


#  Predict on full dataset
y_full_pred = gb_model.predict(X_full_scaled)

#  Evaluate on full dataset (303 records)
print(" Training Accuracy:", accuracy_score(y_train, train_preds))
print(" Test Accuracy:", accuracy_score(y_test, test_preds))
print(" Classification Report on FULL dataset (303 rows):")
print(classification_report(y, y_full_pred))

print(" Confusion Matrix:")
print(confusion_matrix(y, y_full_pred))

"""# Insight :-

Gradient Boosting model has perfect training accuracy (100%) and (80%) test accuracy, showing some overfitting.

# Model with Hyper parameters
"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. Create the model
gb_model = GradientBoostingClassifier(
   n_estimators=30,
    learning_rate=0.02,
    max_depth=4,
    min_samples_split=15,
    min_samples_leaf=10,
    subsample=0.2,
    random_state=42
)

gb_model.fit(X_train_scaled, y_train)

#  Predict on full dataset
X_full_scaled = scaler.transform(X)
train_preds = gb_model.predict(X_train_scaled)
test_preds = gb_model.predict(X_test_scaled)
y_full_pred = gb_model.predict(X_full_scaled)

#  Evaluate on full dataset (303 records)
print(" Training Accuracy:", accuracy_score(y_train, train_preds))
print(" Test Accuracy:", accuracy_score(y_test, test_preds))
print(" Classification Report on FULL dataset (303 rows):")
print(classification_report(y, y_full_pred))

print(" Confusion Matrix:")
print(confusion_matrix(y, y_full_pred))

"""# Insight :-

Your model has almost equal train (85%) and test (83.6%) accuracy, showing it generalizes very well.
It predicts both classes accurately, especially class 1 (disease), with a high recall.
Overall, it’s a well-balanced and reliable model, great for real-world use.

# sample prediction using Gradient Boosting
"""

features = ['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg',
            'thalachh', 'exng', 'oldpeak', 'slope', 'ca', 'thal']
import numpy as np
import warnings
warnings.filterwarnings(action='ignore',category=FutureWarning)
sample_data = np.array([[52, 1, 0, 125, 212, 0, 1, 168, 0, 1.0, 2, 1, 2]])
prediction = gb_model.predict(sample_data)
print("Predicted class:", prediction[0])


probability = gb_model.predict_proba(sample_data)
print("Probability of No Disease:", f"{probability[0][0]:.2f}")
print("Probability of Disease:", f"{probability[0][1]:.2f}")

"""# Result :-

The model predicts no disease, but with medium confidence (62%), meaning it's a borderline case.
The probabilities are close, so the model is not very sure about this prediction.

# 5. XG Boost

XGBoost (Extreme Gradient Boosting) is an optimized and faster version of Gradient Boosting.
It's designed to be highly efficient, accurate, and scalable, especially for large datasets.
Like Gradient Boosting, it builds models in stages — each new tree corrects the errors of the previous ones.

# Why We Use XGBoost:

Gives high accuracy even with default settings

Can handle complex patterns in the data

Performs better than most traditional models (like logistic regression, decision trees)

It supports fine-tuning with many hyperparameters

# Model without Hyperparameters
"""

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

xgb_model = XGBClassifier(random_state=45)

xgb_model.fit(X_train_scaled, y_train)

#  Predict on full dataset
X_full_scaled = scaler.transform(X)
train_preds = xgb_model.predict(X_train_scaled)
test_preds = xgb_model.predict(X_test_scaled)
y_full_pred = xgb_model.predict(X_full_scaled)

#  Evaluate on full dataset (303 records)
print(" Training Accuracy:", accuracy_score(y_train, train_preds))
print(" Test Accuracy:", accuracy_score(y_test, test_preds))
print(" Classification Report on FULL dataset (303 rows):")
print(classification_report(y, y_full_pred))

print(" Confusion Matrix:")
print(confusion_matrix(y, y_full_pred))

"""# Insight :-

The model shows 100% training accuracy but a lower test accuracy of 78.6%, indicating clear overfitting.
However, on the full dataset, it delivers 96% accuracy with excellent precision and recall for both classes.

# Model with Hyper parameters
"""

from xgboost import XGBClassifier
import warnings
warnings.simplefilter(action='ignore',category=FutureWarning)

xgb_model = XGBClassifier(
    n_estimators=60,
    learning_rate=0.03,
    max_depth=2,
    min_child_weight=5,
    subsample=0.5,
    colsample_bytree=0.5,
    reg_alpha=0.1,
    reg_lambda=1,
    eval_metric='logloss',
    random_state=42
)
xgb_model.fit(X_train_scaled, y_train)

#  Predict on full dataset
X_full_scaled = scaler.transform(X)
train_preds = xgb_model.predict(X_train_scaled)
test_preds = xgb_model.predict(X_test_scaled)
y_full_pred = xgb_model.predict(X_full_scaled)

#  Evaluate on full dataset (303 records)
print(" Training Accuracy:", accuracy_score(y_train, train_preds))
print(" Test Accuracy:", accuracy_score(y_test, test_preds))
print(" Classification Report on FULL dataset (303 rows):")
print(classification_report(y, y_full_pred))

print(" Confusion Matrix:")
print(confusion_matrix(y, y_full_pred))

"""# Insight :-


The model shows excellent performance with almost equal training (87.6%) and test (85.2%) accuracy — no overfitting.
Both classes are predicted very well, especially accuracy with 87%.
This is a highly accurate and well-balanced model, ready for real-world use.

"""

import warnings
warnings.filterwarnings(action='ignore',category=FutureWarning)
# cross validation
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(xgb_model, X_full_scaled, y, cv=5, scoring='accuracy')
print("\nCross-Validation Scores (5-fold):", cv_scores)
print("Mean CV Accuracy:", cv_scores.mean())
print("Standard Deviation of CV Accuracy:", cv_scores.std())

# AUC Score
from sklearn.metrics import roc_auc_score
y_probs = xgb_model.predict_proba(X_test_scaled)[:, 1]
auc_score = roc_auc_score(y_test, y_probs)
print("AUC Score:", auc_score)

# ROC Curve
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(y_test, y_probs)

plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.2f}")
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - XGBoost")
plt.legend()
plt.tight_layout()
plt.show()

"""#  Insight:

--> The model achieved a strong mean cross-validation accuracy of 85.13%, indicating reliable generalization performance.

-->  The AUC score of 0.91 suggests excellent discriminative power in distinguishing between patients with and without heart disease.

--> The low standard deviation (0.045) reflects consistent performance across all folds.

--> Overall, the XGBoost model demonstrates high effectiveness and stability, making it a strong candidate for heart disease prediction.

# sample prediction using XG Boosting
"""

features = ['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg',
            'thalachh', 'exng', 'oldpeak', 'slope', 'ca', 'thal']
import numpy as np
sample_data = np.array([[52, 1, 0, 125, 212, 0, 1, 168, 0, 1.0, 2, 1, 2]])
prediction = xgb_model.predict(sample_data)
print("Predicted class:", prediction[0])


probability = xgb_model.predict_proba(sample_data)
print("Probability of No Disease:", f"{probability[0][0]:.2f}")
print("Probability of Disease:", f"{probability[0][1]:.2f}")

"""# Result :-

The XGBoost model predicted class 0, indicating the person is not likely to have heart disease.
It is 77% confident about no disease and 23% probability for having the disease.
Since the probability is significantly in favor of class 0, the prediction can be considered reliable.

# 6. Support Vector Machine

SVM (Support Vector Machine) is a powerful supervised machine learning algorithm used mainly for binary classification.
It works by finding the best boundary (hyperplane) that separates two classes with the maximum margin — meaning it tries to keep the classes as far apart as possible.

# Why We Use SVM:

Gives high accuracy in many classification tasks

Effective even when the data is not linearly separable

Works well with small to medium-sized datasets

Can handle outliers and imbalanced classes with proper tuning

# Model without Hyper Parameters
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# 1. Create the model
svm_model = SVC(random_state=50)

# 2. Train
svm_model.fit(X_train_scaled, y_train)

#  Predict on full dataset
X_full_scaled = scaler.transform(X)
train_preds = svm_model.predict(X_train_scaled)
test_preds = svm_model.predict(X_test_scaled)
y_full_pred = svm_model.predict(X_full_scaled)

#  Evaluate on full dataset (303 records)
print(" Training Accuracy:", accuracy_score(y_train, train_preds))
print(" Test Accuracy:", accuracy_score(y_test, test_preds))
print(" Classification Report on FULL dataset (303 rows):")
print(classification_report(y, y_full_pred))

print(" Confusion Matrix:")
print(confusion_matrix(y, y_full_pred))

"""# Insight :-

Your SVM model has high training (93.4%) and test (85.2%) accuracy, showing strong performance with slight overfitting.
It makes balanced and accurate predictions for both classes, with F1-scores around 0.85.
Overall, it's a reliable and effective model for heart disease prediction.

# Model with HyperParameters
"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# 1. Create the model
svm_model = SVC(
    C=0.20,
    kernel='rbf',
    gamma=0.04,
    probability=True,
    random_state=50
)

# 2. Train
svm_model.fit(X_train_scaled, y_train)

#  Predict on full dataset
X_full_scaled = scaler.transform(X)
train_preds = svm_model.predict(X_train_scaled)
test_preds = svm_model.predict(X_test_scaled)
y_full_pred = svm_model.predict(X_full_scaled)

#  Evaluate on full dataset (303 records)
print(" Training Accuracy:", accuracy_score(y_train, train_preds))
print(" Test Accuracy:", accuracy_score(y_test, test_preds))
print(" Classification Report on FULL dataset (303 rows):")
print(classification_report(y, y_full_pred))

print(" Confusion Matrix:")
print(confusion_matrix(y, y_full_pred))

"""# Insight :-

The model shows good balance between training (87.2%) and test (83.6%) accuracy, meaning it's not overfitting.
Precision and recall are almost equal for both classes, showing consistent and fair predictions.
Overall, it's a stable and accurate model, suitable for heart disease prediction tasks.

# sample prediction using  Support Vector Machine
"""

features = ['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg',
            'thalachh', 'exng', 'oldpeak', 'slope', 'ca', 'thal']
import numpy as np
sample_data = np.array([[52, 1, 0, 125, 212, 0, 1, 168, 0, 1.0, 2, 1, 2]])
prediction = svm_model.predict(sample_data)
print("Predicted class:", prediction[0])


probability = svm_model.predict_proba(sample_data)
print("Probability of No Disease:", f"{probability[0][0]:.2f}")
print("Probability of Disease:", f"{probability[0][1]:.2f}")

"""# Result :-

The model predicts no disease with high confidence (83%), meaning it clearly leans toward a healthy outcome.
The low disease probability (17%) shows the input strongly matches non-disease patterns.
This is a confident and trustworthy prediction from the model.

# Showing Accuracy in Plotting
"""

import matplotlib.pyplot as plt

models = ['SVM', 'Gradient Boosting', 'Decision Tree', 'XGBoost', 'Logistic Regression', 'Random Forest']
accuracies = [86, 85, 86, 87, 86, 85]

plt.figure(figsize=(10,6))
plt.barh(models, accuracies, color='skyblue')
plt.xlabel("Accuracy (%)")
plt.title("Model Accuracies for Heart Disease Prediction")

for index, value in enumerate(accuracies):
    plt.text(value + 0.5, index, f"{value}%", va='center')

plt.xlim(70, 95)
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""# Observation :-

Despite applying hyperparameter tuning, XG boosting achieved the highest accuracy at 87%, indicating strong linear separability in the dataset. Random Forest and Gradient Boosting performed consistently at 85%, showing limited improvement even after tuning, likely due to feature constraints or noise. Decision Tree also performing well after tuning with 86% and it was overfitted before tuning. Overall, the dataset may have accuracy limitations, and further improvement may require feature engineering or ensemble stacking rather than just model tuning.

# Showing Accuracy in Dataframe
"""

data = {
    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting', 'XGBoost', 'SVM'],
    'Accuracy': [0.86, 0.86, 0.85, 0.85, 0.87, 0.86],
    'Macro F1-Score': [0.86,0.86,0.85,0.85,0.87,0.86]
}

df = pd.DataFrame(data)
print(df)

"""# Conclusion :-

The XG boost model demonstrates a commendable capability in recognizing potential heart patients.
Overall, ensemble models like SVM, Random Forest,Gradient boosting and logistic regression  delivered the best balance between accuracy and generalization and offered simplicity with strong performance. These models can be valuable tools in early heart disease detection to support medical decision-making.

# Deployment
"""

import pickle

with open("model.pkl", "wb") as model_file:
    pickle.dump(xgb_model, model_file)

# Save the fitted scaler
with open("scaler.pkl", "wb") as scaler_file:
    pickle.dump(scaler, scaler_file)